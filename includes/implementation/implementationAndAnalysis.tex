Practice within the discipline of Data Science is at least as much engineering as it is science, if not more. For most projects, the typical pipeline includes extraction of data from some source and cleaning and reformatting of that data; there is typically feature engineering involved, which is the process of designing higher-level features based on a raw or semi-raw extract of data. After that comes outlier detection and typically also some variable transformation in order to balance the scales and distributions of signals in the data. All of these steps are called preprocessing, but will often require more work than any other task in the pipeline, simply because the preprocessing constitutes a bottelneck to the quality of the scientific analysis, where errors made in the preprocessing result in poor or, even worse, misleading results. Because of this, preprocessing must be carried out with the desired analysis in mind, and analysis must be conducted with awareness of the problems that might result from errors in preprocessing.

In this work, a preprocessing pipeline is designed based on principles from software development. Given a timeframe and time constraints such as what days of the week and hours of the day to use, it enables the researcher to produce processed datasets for analysis. The pipeline is not designed as a single endpoint, but uses both IPython Notebooks and custom modules to allow the researcher to directly manipulate parts of the implementation such as to make it easier to produce the desired processed datasets. 

In the following chapter the preprocessing pipeline and methods used in analysis is described. An important source of error in data science is the fact that the analyst often has to make method and parameter choices based on intuition and heuristics where computation time renders analysis non-feasible, hence, the language used may sound argumentative at times. [SHOULD CONTAIN A BUNCH OF REFERENCES TO SECTIONS] This chapter first details assumptions about the data, how the data is extracted and preprocessed and how it is used to create higher-level features, or \textit{indicators} as they will henceforth be referred to. Threshold filtering, outlier-detection and variable transformation are explained, and choises made along the way are discussed. The analysis part of the pipeline is threefold. The first part deals with supervised learning where personality traits are predicted from behavioral indicators. The second part revolves around understanding 